{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing The Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I am using typical data science stack: numpy, pandas, sklearn, matplotlib, seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#numpy and pandas for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import median\n",
    "from scipy.stats import norm\n",
    "import re\n",
    "import math\n",
    "\n",
    "#matplotlib and seaborn for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "import plotly\n",
    "from plotly.offline import iplot\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#file system management\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from lightgbm import LGBMRegressor\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/mercari_train.csv.gz', compression='gzip',\n",
    "                   error_bad_lines=False)\n",
    "test_data = pd.read_csv('../data/mercari_test.csv.gz', compression='gzip',\n",
    "                   error_bad_lines=False)\n",
    "# train_data.to_csv(r'../Solutions/train.csv', index = False)\n",
    "# test_data.to_csv(r'../Solutions/test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For EDA purposes train_data will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lets examine the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of data which is null in each column\n",
    "(df.isna().sum() / df.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns brand_name and item_description has missing values. I will have to fill in these missing values which is known as imputation. 31.38 % of brand_name values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights\n",
    "1) Most of names are unique.\n",
    "2) Top Category is Women Apparels.\n",
    "3) Top Brand is PINK.\n",
    "4) New is the most common product description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicaterows = df[df.duplicated(['name', 'item_condition_id', 'category_name', 'brand_name', 'shipping', 'price', 'item_description'])]\n",
    "\n",
    "duplicaterows[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.name == 'Bombshell') & (df.price == 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.name == 'Nike slides') & (df.price == 12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.name == 'Victoria secret pink') & (df.price == 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.name == 'bundle') & (df.price == 23)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the Distribution of Target Variable - Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(range(df.shape[0]), np.sort(df.price.values))\n",
    "plt.xlabel('Index', fontsize = 12)\n",
    "plt.ylabel('Price', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of products with price less than $3 -', df['price'][df['price'] < 3].count())\n",
    "print('Number of products with price less than $4 -', df['price'][df['price'] < 4].count())\n",
    "print('Number of products with price greater than $800 -', df['price'][df['price'] > 800].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some outliers in the data on the upper side. So Pricing of any product can be between 4 to 800 dollars. So I will remove 2026 rows from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.price >= 4) & (df.price <= 800)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].plot.hist(bins=50, figsize=(12,6), edgecolor='white', range=[0,500])\n",
    "plt.title('Price Distribution', fontsize=12)\n",
    "plt.xlabel('Price', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price distribution is positively skewed. Max values lie between $4 and \\$100.We will take the logarithm to see if the Log(price) is normally distributed or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.distplot(np.log1p(df['price']), bins=50, kde=True, fit=norm)\n",
    "plt.title('Log(Price) Distribution', fontsize=12)\n",
    "plt.xlabel('log(price+1)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After taking logarithm of target variable Price, it appears to be almost almost normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price.describe(percentiles = [0.8, 0.9, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 95% of the items are priced below $65\n",
    "* 99% of the items are priced below $125\n",
    "* 1% of the products might be outliers or some expensive products\n",
    "* Mean price is $23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new feature - log(price)\n",
    "df['log_price'] = np.log1p(df['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate and Multivariate Analysis\n",
    "### Shipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['shipping'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of products with shipping equals 0 (or shipping fee paid by buyer) is greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipping_buyer = df[df['shipping'] == 0]['price']\n",
    "shipping_seller = df[df['shipping'] == 1]['price']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(shipping_seller, alpha=0.5, bins=50, range=[0,100], label='seller')\n",
    "plt.hist(shipping_buyer, alpha=0.5, bins=50, range=[0,100], label='buyer')\n",
    "plt.xlabel('Price')\n",
    "plt.legend(fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Median price when shipping is paid by seller is ${}'.format(shipping_seller.median()))\n",
    "print('Median price when shipping is paid by buyer is ${}'.format(shipping_buyer.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.boxplot(x='shipping', y='log_price', data=df)\n",
    "plt.title('Effect of shipping on log(price)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Not able to explain the condition derived above.\n",
    "* But it should be like the price of products should be a bit more when the shipping is paid by buyer than seller. It should explain that buyer pay more shipping fee and the total product cost goes up. But here when shipping is paid by seller the price is high. So cannot draw any insights from this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.brand_name.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.brand_name.value_counts()[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_brand_name'] = (df['brand_name'].isna()).astype(np.int8) #if brand_name is present, 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_count = df['has_brand_name'].sum()\n",
    "true_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.boxplot(x='has_brand_name', y='log_price', data=df)\n",
    "plt.title('Effect of shipping on log_price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Median price of products with missing brand names is lower than products having brand name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expensive Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.where(df['price'] > 100).sort_values(by=['price'], ascending=False)\n",
    "\n",
    "brands = data.groupby('brand_name')['price'].agg(['count', 'mean']).sort_values(by=['count'], ascending=False).reset_index()\n",
    "\n",
    "expensive_brands = brands[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "plt.barh(range(0,len(expensive_brands)), expensive_brands['mean'], align='center', alpha=0.5, color='r')\n",
    "\n",
    "plt.yticks(range(0,len(expensive_brands)), expensive_brands['brand_name'])\n",
    "plt.xticks()\n",
    "plt.title('Mean price of 20 expensive brands')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Brands')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These brands have products which are expensive (or over $100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luxurious Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.where(df['price'] > 500).sort_values(by=['price'], ascending=False)\n",
    "\n",
    "brands = data.groupby('brand_name')['price'].agg(['count', 'mean']).sort_values(by=['count'], ascending=False).reset_index()\n",
    "\n",
    "luxurious_brands = brands[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "plt.barh(range(0,len(luxurious_brands)), luxurious_brands['mean'], align='center', alpha=0.5, color='b')\n",
    "\n",
    "plt.yticks(range(0,len(luxurious_brands)), luxurious_brands['brand_name'])\n",
    "plt.xticks()\n",
    "plt.title('20 luxurious brands')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Brands')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These brands have products which are luxurious (or over $500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheap Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.where(df['price'] < 50).sort_values(by=['price'], ascending=False)\n",
    "\n",
    "brands = data.groupby('brand_name')['price'].agg(['count', 'mean']).sort_values(by=['count'], ascending=False).reset_index()\n",
    "\n",
    "cheap_brands = brands[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheap_brands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have got the expensive, luxurious and cheap brands. But the brands in these categories can have products under $100 but they also have products which are expensive or luxurious.\n",
    "* There are 1083 unique brands in the training set\n",
    "* Pink and LuLaRoe are the brands those products are mostly sold.\n",
    "* There are 20448 products that do not have brand name present.\n",
    "* Amongst the top selling 10 brands on mercari, it seems that products of these brands are not expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category_name.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category_name.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category_name.value_counts()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 2 sub-categories under the category_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_category(input_data):\n",
    "    \"\"\"\n",
    "    Split the category_name into 3 parts as category_0, category_1 and category_2\n",
    "    \"\"\"\n",
    "    for i in range(3):\n",
    "        \n",
    "        def get_categories(ele):\n",
    "            \n",
    "            if type(ele) != str:\n",
    "                return np.nan\n",
    "        \n",
    "            cat = ele.split('/')\n",
    "            \n",
    "            if i >= len(cat):\n",
    "                return np.nan\n",
    "            else:\n",
    "                return cat[i]\n",
    "\n",
    "        col_name = 'category_' + str(i)\n",
    "        \n",
    "        input_data[col_name] = input_data['category_name'].apply(get_categories)\n",
    "        \n",
    "        input_data.fillna({'category_name': 'Other'}, inplace = True)\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #splitting category_name into category_0, category_1 and category_2\n",
    "df = process_category(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} unique values in category_0'.format(len(df['category_0'].unique())))\n",
    "print('There are {} unique values in category_1'.format(len(df['category_1'].unique())))\n",
    "print('There are {} unique values in category_2'.format(len(df['category_2'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,12))\n",
    "sns.boxplot(x = 'log_price', y = 'category_0', data = df, orient = 'h')\n",
    "plt.title('Boxplot of categories and prices', fontsize=14)\n",
    "plt.xlabel('Log(Price)', fontsize=14)\n",
    "plt.ylabel('Categories', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nothing available to compare as the first category only has Women tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Subcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_mean = df.groupby('category_1')['price'].agg(['mean']).sort_values(by=['mean'], ascending = False).reset_index()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 14))\n",
    "\n",
    "plt.barh(range(0,len(cat_mean)), cat_mean['mean'], align='center', alpha=0.5, color='b')\n",
    "\n",
    "plt.yticks(range(0,len(cat_mean)), cat_mean['category_1'])\n",
    "plt.xticks()\n",
    "plt.title('Mean price of items grouped by 20 first sub-categories')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Womens Handbag have highest mean price.\n",
    "* Underwear has lowest mean price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Subcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_cat_mean = df.groupby('category_2')['price'].agg(['mean']).sort_values(by=['mean'], ascending = False).reset_index()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 14))\n",
    "\n",
    "plt.barh(range(0,len(sub_cat_mean)), sub_cat_mean['mean'], align='center', alpha=0.5, color='g')\n",
    "\n",
    "plt.yticks(range(0,len(sub_cat_mean)), sub_cat_mean['category_2'])\n",
    "plt.xticks()\n",
    "plt.title('Mean price of items grouped by 20 second sub-categories')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Shoulder bags have highest mean price.\n",
    "* Bra's have lowest mean price in third category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Women/Athletic Apparel/Pants, Tights, Leggings has the highest number of products on the marketplace.\n",
    "* There are 23 unique categories.\n",
    "* There are 0 missing values.\n",
    "* I have cleaned data in category_name feature. I have removed slashes and made them into 3 categories or 3 columns for category and its sub-categories. This is done so that while modelling these subcategory can be used to predict the price as they are very improtant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.stripplot(df['item_condition_id'], df['price'], palette = 'Reds')\n",
    "plt.title(\"Item Condition vs Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['item_condition_id'].value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.boxplot(x='item_condition_id', y='log_price', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('item_condition_id')['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As per assumption, most sellers tend to give item_condition_id as 2,3 as buyers don't buy products which are not in great condition.\n",
    "* But there are many products that are in great condition as significant sellers have given item_condition_id = 1 to the products, and as expected their mean price is also higher because if the product is in great condition its price should be high.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now I plan to do Text analysis of the column containing text, I tried roughly without them and tried predicting the price , but it came to be a poor prediction, Hence I am thinking to use Language processing (NLP) to make some sense out of the data present in text form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.name.values[10])\n",
    "print('-'*50)\n",
    "print(df.name.values[50])\n",
    "print('-'*50)\n",
    "print(df.name.values[500])\n",
    "print('-'*50)\n",
    "print(df.name.values[1000])\n",
    "print('-'*50)\n",
    "print(df.name.values[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique product names in the dataset are {:.2f}%'.format(df['name'].str.lower().str.strip().nunique() / df.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "cloud = WordCloud(width=3000, height=2000).generate(' '.join(df.name.astype(str)))\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top words seems to be brand names such as Victoria Secret, American Eagle,which are brand names as people tends to keep them in the product title. Same can be done with product description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_name'] = df['name'].apply(lambda x: len(x))\n",
    "\n",
    "df.groupby('len_name')['price'].agg(['mean', 'median'])[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I cannot derive any insights from here, like any relation between the word length and average price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords without no, not, etc\n",
    "STOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(sentence):\n",
    "    \"\"\"\n",
    "    Remove emojis from the string\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return pattern.sub(r'', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    \"\"\"\n",
    "    Expand and create common english contractions in the text\n",
    "    \"\"\"\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(input_data, cols):\n",
    "    \"\"\"\n",
    "    Take the text columns and process the data. Expand contractions, use regex to remove symbols/numbers, remove emojis, punctuations\n",
    "    and stopwords and convert text to lowercase\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        \n",
    "        processed_data = []\n",
    "        \n",
    "        for sent in input_data[col].values:\n",
    "            \n",
    "            sent = decontracted(sent)\n",
    "            sent = sent.replace('\\\\r', ' ')\n",
    "            sent = sent.replace('\\\\\"', ' ')\n",
    "            sent = sent.replace('\\\\n', ' ')\n",
    "            sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "            sent = remove_emoji(sent)\n",
    "            sent = ' '.join(e for e in sent.split() if e not in STOPWORDS)\n",
    "            processed_data.append(sent.lower().strip())\n",
    "            \n",
    "        input_data[col] = processed_data\n",
    "        \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp features\n",
    "def get_text_features(input_data):\n",
    "    \"\"\"\n",
    "    NLP features derived from the text columns\n",
    "    \"\"\"\n",
    "    input_data['has_brand_name'] = (input_data['brand_name'].isnull()).astype(np.int8) #if brand_name is present, 1 else 0\n",
    "    \n",
    "    input_data['has_price'] = np.where(input_data['item_description'].str.contains(' rm ', na = False), 1, 0) #if item_description has [rm] which is price string removed, 1 else 0\n",
    "\n",
    "    input_data['reversed_item_condition_id'] = 5 - input_data['item_condition_id']\n",
    "\n",
    "    input_data['is_luxurious'] = (input_data['brand_name'].isin(luxurious_brands['brand_name'])).astype(np.int8)\n",
    "\n",
    "    input_data['is_expensive'] = (input_data['brand_name'].isin(expensive_brands['brand_name'])).astype(np.int8)\n",
    "\n",
    "    input_data['is_cheap'] = (input_data['brand_name'].isin(cheap_brands['brand_name'])).astype(np.int8)\n",
    "\n",
    "    input_data['len_name'] = input_data['name'].str.len()\n",
    "    input_data['len_item_description'] = input_data['item_description'].str.len()\n",
    "    input_data['len'] = input_data['len_name'] + input_data['len_item_description']\n",
    "\n",
    "    input_data['token_count_name'] = input_data['name'].apply(lambda x: len(x.split(' ')))\n",
    "    input_data['token_count_item_description'] = input_data['item_description'].apply(lambda x: len(x.split(' ')))\n",
    "    input_data['token_count'] = input_data['token_count_name'] + input_data['token_count_item_description']\n",
    "    input_data['token_count_ratio'] = input_data['token_count_name']/input_data['token_count_item_description']\n",
    "\n",
    "    input_data[\"name_words\"] = input_data[\"name\"].str.count(\"(\\s|^)[a-z]+(\\s|$)\")\n",
    "    input_data[\"item_description_words\"] = input_data[\"item_description\"].str.count(\"(\\s|^)[a-z]+(\\s|$)\")\n",
    "    input_data[\"words\"] = input_data[\"name_words\"] + input_data[\"item_description_words\"]\n",
    "\n",
    "    input_data[\"name_numbers\"] = input_data[\"name\"].str.count(\"(\\s|^)[-+]?[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?(\\s|$)\")\n",
    "    input_data[\"item_description_numbers\"] = input_data[\"item_description\"].str.count(\"(\\s|^)[-+]?[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?(\\s|$)\")\n",
    "    input_data[\"numbers\"] = input_data[\"name_numbers\"] + input_data[\"item_description_numbers\"]\n",
    "\n",
    "    input_data[\"name_letters\"] = input_data[\"name\"].str.count(\"[a-zA-Z]\")\n",
    "    input_data[\"item_description_letters\"] = input_data[\"item_description\"].str.count(\"[a-zA-Z]\")\n",
    "    input_data[\"letters\"] = input_data[\"name_letters\"] + input_data[\"item_description_letters\"]\n",
    "\n",
    "    input_data[\"name_digits\"] = input_data[\"name\"].str.count(\"[0-9]\")\n",
    "    input_data[\"item_description_digits\"] = input_data[\"item_description\"].str.count(\"[0-9]\")\n",
    "    input_data[\"digits\"] = input_data[\"name_digits\"] + input_data[\"item_description_digits\"]\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(input_data):\n",
    "    \"\"\"\n",
    "    Fills the nan/missing values with 'missing' for text columns\n",
    "    \"\"\"\n",
    "    input_data.fillna({'name': 'missing', 'item_description': 'missing'}, inplace=True)\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_data):\n",
    "    \"\"\"\n",
    "    Process the data by handling missing values, process category_name, process text\n",
    "    \"\"\"\n",
    "    input_data = input_data[(input_data['price'] >= 3) & (input_data['price'] <= 800)]\n",
    "    \n",
    "    input_data['price'] = np.log1p(input_data['price'])\n",
    "\n",
    "    input_data = handle_missing_values(input_data)\n",
    "    \n",
    "    input_data = process_category(input_data)\n",
    "    \n",
    "    input_data = process_text(input_data, ['name', 'item_description', 'category_name'])\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess(df)\n",
    "data.fillna({'category_0': 'other', 'category_1': 'other', 'category_2': 'other'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brand Names in Product Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = df['brand_name'].unique().tolist()\n",
    "name = df['name'].tolist()\n",
    "name_list = [i.strip(',').split(' ') for i in name]\n",
    "name_corpus = [item for name in name_list for item in name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = set(brands)\n",
    "brands_in_name = list(brands.intersection(name_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=3000, height=2000).generate(' '.join(brands_in_name))\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The temp object here is a pandas.series object which does not have a iplot method when not linked to plotly. \n",
    "#We need cufflinks to link plotly to pandas and add the iplot method.\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(corpus, n = 20):\n",
    "    vect = CountVectorizer().fit(corpus)\n",
    "    bow = vect.transform(corpus)\n",
    "    sum_words = bow.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vect.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(corpus, n = 20):\n",
    "    vect = CountVectorizer(ngram_range=(2,2)).fit(corpus)\n",
    "    bow = vect.transform(corpus)\n",
    "    sum_words = bow.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vect.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_words(data['name'], 10)\n",
    "df_words = pd.DataFrame(common_words, columns = ['name' , 'count'])\n",
    "df_words.groupby('name').sum()['count'].sort_values(ascending=False).iplot(\n",
    "    kind='bar', \n",
    "    yTitle='Count', \n",
    "    linecolor='black', \n",
    "    title='Top 10 words in product name after preprocessing data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lularose and pink are used in product name mostly which are brand name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_bigrams(data['name'], 10)\n",
    "\n",
    "df_words = pd.DataFrame(common_words, columns = ['name' , 'count'])\n",
    "\n",
    "df_words.groupby('name').sum()['count'].sort_values(ascending=False).iplot(\n",
    "    kind='bar', \n",
    "    yTitle='Count', \n",
    "    linecolor='black', \n",
    "    title='Top 10 bi-grams in product name after preprocessing data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.item_description.values[101])\n",
    "print('-'*50)\n",
    "print(df.item_description.values[60])\n",
    "print('-'*50)\n",
    "print(df.item_description.values[590])\n",
    "print('-'*50)\n",
    "print(df.item_description.values[1020])\n",
    "print('-'*50)\n",
    "print(df.item_description.values[10800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=3000, height=2000).generate(' '.join(df.item_description.astype(str)))\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brand Names in Item Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = df['brand_name'].unique().tolist()\n",
    "name = df['item_description'].tolist()\n",
    "df['item_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = set(brands)\n",
    "brands_in_name = list(brands.intersection(name_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=3000, height=2000).generate(' '.join(brands_in_name))\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_words(data['item_description'], 10)\n",
    "df_words = pd.DataFrame(common_words, columns = ['text' , 'count'])\n",
    "\n",
    "df_words.groupby('text').sum()['count'].sort_values(ascending=False).iplot(\n",
    "    kind='bar', \n",
    "    yTitle='Count', \n",
    "    linecolor='black', \n",
    "    title='Top 10 words in product description after preprocessing data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_bigrams(data['item_description'], 10)\n",
    "df_words = pd.DataFrame(common_words, columns = ['text' , 'count'])\n",
    "\n",
    "df_words.groupby('text').sum()['count'].sort_values(ascending=False).iplot(\n",
    "    kind='bar', \n",
    "    yTitle='Count', \n",
    "    linecolor='black', \n",
    "    title='Top 10 bi-grams in product description after preprocessing data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final dataset prepared for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP features\n",
    "data = get_text_features(data)\n",
    "\n",
    "data.fillna({'brand_name': ' '}, inplace = True)\n",
    "\n",
    "#concatenate text features\n",
    "data['name'] = data['name'] + ' ' + data['brand_name'] + ' ' + data['category_name']\n",
    "data['text'] = data['name'] + ' ' + data['item_description']\n",
    "\n",
    "data = data.drop(columns = ['brand_name', 'item_description', 'category_name'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = data[['price', 'item_condition_id',\n",
    "       'shipping', 'category_0', 'category_1',\n",
    "       'category_2', 'has_brand_name', 'has_price',\n",
    "       'reversed_item_condition_id', 'is_luxurious', \n",
    "        'is_expensive']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatrix  = df1.corr()\n",
    "plt.figure(figsize = (18,9))\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = data[['price', 'is_cheap', 'len_name', 'len_item_description', 'len',\n",
    "       'token_count_name', 'token_count_item_description', 'token_count',\n",
    "       'token_count_ratio', 'name_words', 'item_description_words', 'words',\n",
    "       'name_numbers', 'item_description_numbers', 'numbers', 'name_letters']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatrix  = df2.corr()\n",
    "plt.figure(figsize = (18,9))\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = data[['price', 'item_description_letters', 'letters', 'name_digits',\n",
    "       'item_description_digits', 'digits']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatrix  = df3.corr()\n",
    "plt.figure(figsize = (18,9))\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions/Observations from Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I have found strong correlation between is_luxurious. is_expensive and prices so I will use these columns separtely while modelling as they can lead to price prediction results.\n",
    "* So is_expensive, is_luxurious seems to have high predictive power.\n",
    "* Other NLP features seems to be uncorrelated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine leaning (Modelling) - Present in model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ridge regression with best hyperparameters takes very less time to train and rmsle is also less than 0.5, so I choose ridge_model to predict the outcome of test_data or mercari_test"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7db60af8344ad1c87e22d8d26847c385c2d05c7c42746d8d5b74cd2f95474f78"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
